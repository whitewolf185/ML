\section{Выводы}
В ходе выполнения лабораторной работы я познакомился с линейными моделями классического машинного обучения: логистической регрессией, 
методом опорных векторов, наивным байесовским классификатором и методом k-ближайших соседей. Больше всего заинтересовал алгоритм 
k-ближайших соседей для классификации, так как он кажется интуитивно лучшим, ведь позволяет разделять классы не прямой, как в случае SVM,
а проводить границу, фактически, по точкам. Однако его недостаток в большой вычислительной сложности и в том, что если нет большой корреляции 
между признаком и результатом, то алгоритм банально будет часто ошибаться.

Основная сложность в работе --- реализация каждого метода вручную, пришлось искать очень много методов из библиотек numpy и scikit-learn, 
чтобы легко работать с данными. Пожалуй, это заняло большую часть времени.

В результате набор данных Rice type получилось разделить линейными моделями с поразительной точностью 98-99\%. Точность такая высокая, 
потому что изначально данные очень хорошо делились, к тому же были удалены выбросы, которые могли мешать обучению.

Особенно хочется отметить, что такой точности получается добиться далеко не всегда, потому что часто бывает так, что между
признаками и результатом нет такой большой корреляции, как в выбранном датасете. Предположу, что вместо применения алгоритмов машинного обучения
можно было с таким же успехом описать алгоритм, который относил бы рис к одному или другому типу на основе жестко заданных границ признаков.
\pagebreak
